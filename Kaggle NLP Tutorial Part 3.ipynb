{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle NLP Tutorial: [Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)\n",
    "-----\n",
    "# Part 3: More Fun With Word Vectors\n",
    "# 1. Numeric Representations of Words\n",
    "Each word is represented as a vector with N numerical values (here N=300).\n",
    "\n",
    "The Word2Vec model trained in Part 2 consists of a feature vector for each word in the vocabulary, stored in a numpy array called **`syn0`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model that we created in Part 2\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(16490, 300)\n"
     ]
    }
   ],
   "source": [
    "print type(model.syn0)\n",
    "print model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. \n",
    "- The number of rows: the number of words in the model's vocabulary\n",
    "- The number of columns: the size of the feature vector, which we set in Part 2\n",
    "\n",
    "Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.63045412e-02,  -7.70469010e-02,   5.10520190e-02,\n",
       "         1.04793541e-01,  -8.05942249e-03,   1.42707271e-04,\n",
       "         4.70351242e-02,   3.08730341e-02,   1.56136565e-02,\n",
       "         8.89217481e-02,  -7.53695518e-02,  -5.59975766e-03,\n",
       "         5.96253527e-03,   6.15217909e-03,  -9.69854835e-03,\n",
       "         8.10943637e-03,  -2.35759504e-02,  -3.36211994e-02,\n",
       "        -2.19760146e-02,   3.30724761e-05,  -4.15189900e-02,\n",
       "         1.42527252e-01,   3.37156020e-02,   8.60805437e-02,\n",
       "        -2.27182470e-02,  -7.99016654e-03,  -7.55509958e-02,\n",
       "        -1.01256482e-01,   3.30181271e-02,  -1.27396137e-01,\n",
       "        -1.47526279e-01,  -7.87535012e-02,   5.81267923e-02,\n",
       "        -8.56401306e-03,  -1.24435067e-01,   9.53752324e-02,\n",
       "        -1.64249502e-02,  -6.64357329e-03,   7.04739988e-02,\n",
       "         2.08331924e-02,   2.15384346e-02,   3.17606106e-02,\n",
       "        -6.17580637e-02,  -8.08355287e-02,   3.97209302e-02,\n",
       "        -3.23512033e-02,   4.61595096e-02,  -3.46926646e-03,\n",
       "        -1.85832626e-03,  -2.57842243e-02,   8.31811316e-03,\n",
       "        -1.36357555e-02,  -4.29453738e-02,   1.90274883e-02,\n",
       "         2.50086244e-02,   5.73607907e-02,   1.22772753e-02,\n",
       "        -8.61042887e-02,  -3.41404863e-02,   4.89157857e-03,\n",
       "        -3.61402035e-02,  -1.08634941e-01,  -4.00400860e-03,\n",
       "        -2.17444971e-02,  -3.61074246e-02,  -1.24389246e-01,\n",
       "         2.24137791e-02,   5.30211069e-03,   8.90024304e-02,\n",
       "         1.57642495e-02,  -6.74984604e-02,   1.66927967e-02,\n",
       "         1.12204820e-01,  -3.32722366e-02,   7.60075375e-02,\n",
       "         4.38351743e-02,   7.64547437e-02,  -3.85121908e-03,\n",
       "        -3.46711911e-02,   7.33452290e-02,   2.62874477e-02,\n",
       "         7.27106817e-04,   3.17196809e-02,  -1.58356577e-02,\n",
       "        -2.93927942e-03,  -1.73027453e-03,   3.45016643e-02,\n",
       "        -8.85432437e-02,  -8.83812457e-03,  -2.94242636e-03,\n",
       "        -9.01991203e-02,   2.84205340e-02,  -7.95294493e-02,\n",
       "        -1.72499925e-01,   2.01893579e-02,   7.46510783e-03,\n",
       "         3.86068365e-04,   2.67321058e-02,   3.83744761e-02,\n",
       "         5.43241687e-02,   1.16876580e-01,  -1.46092894e-02,\n",
       "        -3.51843648e-02,  -1.31252194e-02,   2.06951424e-02,\n",
       "        -7.54301175e-02,   1.00798476e-02,  -1.23566287e-02,\n",
       "         7.23302737e-02,  -6.14617094e-02,   4.26026732e-02,\n",
       "         8.39083418e-02,   8.46523270e-02,   6.45450279e-02,\n",
       "         4.40777838e-02,   4.27236669e-02,  -4.85488065e-02,\n",
       "        -1.68097038e-02,  -7.30979368e-02,   2.54565924e-02,\n",
       "        -3.07342820e-02,  -9.08219162e-03,   5.74009866e-03,\n",
       "         5.96086085e-02,   9.48691461e-03,   1.18859196e-02,\n",
       "        -1.09804503e-03,   6.63756803e-02,   5.12354709e-02,\n",
       "        -4.33724113e-02,   2.57025193e-02,   5.71251996e-02,\n",
       "         4.10324968e-02,  -1.78173706e-02,  -4.94057918e-03,\n",
       "        -6.65633902e-02,  -6.09187596e-02,   3.03246342e-02,\n",
       "         3.83974724e-02,   1.49919409e-02,   7.52269402e-02,\n",
       "        -4.41272035e-02,  -1.24408016e-02,  -2.24181004e-02,\n",
       "         2.99348682e-02,  -6.76640868e-02,  -5.85014559e-02,\n",
       "         1.14012212e-02,  -1.08744174e-01,  -4.86934296e-04,\n",
       "         8.61744434e-02,   7.47660827e-03,   1.00224158e-02,\n",
       "         2.72378791e-02,  -5.54056466e-02,   4.67230342e-02,\n",
       "        -1.52680064e-02,  -5.55258729e-02,   1.29174562e-02,\n",
       "        -1.51164050e-03,   2.68904995e-02,   3.24466489e-02,\n",
       "         2.77546942e-02,   3.13660540e-02,  -1.25638358e-02,\n",
       "         7.73912668e-02,  -1.42963072e-02,   1.23447962e-02,\n",
       "         4.01259996e-02,   4.91186418e-02,  -7.66601339e-02,\n",
       "        -6.82061771e-03,   7.09961355e-03,  -8.41928348e-02,\n",
       "         8.84381905e-02,   1.12356171e-01,  -4.77752537e-02,\n",
       "         9.59130973e-02,   7.22648576e-02,  -7.40514994e-02,\n",
       "         7.03629013e-03,  -1.22660607e-01,   4.91693281e-02,\n",
       "         1.43472433e-01,   6.26909286e-02,  -1.28175914e-01,\n",
       "        -1.50237605e-02,  -1.31583154e-01,   1.99331380e-02,\n",
       "        -5.62610105e-02,   4.64201532e-02,  -1.30496278e-01,\n",
       "        -6.93475502e-03,   5.28189950e-02,   6.62231892e-02,\n",
       "        -6.39137905e-03,   6.32424727e-02,  -1.02464743e-01,\n",
       "        -1.95802059e-02,   3.27834561e-02,  -3.28639932e-02,\n",
       "         1.85081623e-02,  -2.12839153e-02,  -8.41757134e-02,\n",
       "         8.40935037e-02,   1.00550525e-01,   4.94536869e-02,\n",
       "        -8.71001091e-03,   1.34739268e-03,   1.14575289e-02,\n",
       "        -2.74067186e-02,  -6.15837015e-02,  -3.06563936e-02,\n",
       "         8.95297155e-02,   6.72624782e-02,  -3.13993245e-02,\n",
       "        -3.90796214e-02,  -5.60825039e-03,   6.42579421e-02,\n",
       "        -5.77905029e-02,   3.84957083e-02,  -1.43726375e-02,\n",
       "        -5.10357991e-02,  -6.69877008e-02,   1.03823468e-01,\n",
       "         7.43326917e-02,   1.97096914e-02,  -5.44210747e-02,\n",
       "        -7.48132393e-02,  -5.39544690e-03,  -2.60460712e-02,\n",
       "         8.15685466e-02,   6.48653805e-02,  -9.66549069e-02,\n",
       "         6.25341535e-02,   2.55807023e-02,  -2.09826361e-02,\n",
       "        -1.17316898e-02,   8.94845203e-02,  -1.44787170e-02,\n",
       "         7.70779699e-03,   5.25547564e-02,  -5.71032912e-02,\n",
       "        -6.42619058e-02,   5.74386865e-02,   2.72934996e-02,\n",
       "        -1.69287920e-02,   1.05626471e-02,   4.25655358e-02,\n",
       "        -5.12699783e-02,   1.15787104e-01,  -9.35707986e-02,\n",
       "        -1.20992539e-02,   6.85835630e-02,  -5.69137260e-02,\n",
       "        -7.22514093e-02,   2.14794874e-02,  -2.04331893e-02,\n",
       "         3.39132585e-02,  -3.28841172e-02,   4.34480272e-02,\n",
       "        -3.54176797e-02,  -1.11055739e-01,  -6.72101676e-02,\n",
       "        -3.54352482e-02,  -5.14289271e-03,   7.47178271e-02,\n",
       "         8.86300653e-02,   4.57866117e-02,   3.41247469e-02,\n",
       "        -1.11975912e-02,  -4.68577109e-02,   5.20109423e-02,\n",
       "         1.54252886e-03,  -9.24004391e-02,   6.94314986e-02,\n",
       "        -8.96619484e-02,  -2.75490038e-06,   2.05768980e-02,\n",
       "         6.06569983e-02,  -6.30644411e-02,  -3.40333357e-02,\n",
       "         1.32835582e-01,  -4.44445340e-03,   5.31395301e-02,\n",
       "        -3.44930589e-02,  -3.56947705e-02,   1.59695596e-02,\n",
       "        -4.47025225e-02,  -2.59165261e-02,   2.37744916e-02,\n",
       "         6.92368345e-03,   6.00690348e-03,   1.70441762e-01,\n",
       "         2.85195396e-03,   6.52155131e-02,   1.17675200e-01,\n",
       "        -2.24613138e-02,   5.93797164e-03,   9.23278334e-04], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. From Words To Paragraphs, Attempt 1: Vector Averaging\n",
    "## The IMDB dataset has variable-length reviews.\n",
    "- We need to find a way to **take individual word vectors and transform them into a feature set that is the same length for every review.**\n",
    "- Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. \n",
    "- One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_features = 10\n",
    "featureVec = np.zeros((num_features,),dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the average paragraph vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\" \n",
    "        Function to average all of the word vectors in a given paragraph\n",
    "        - input(words): words in a review, we loop over each word in a review, \n",
    "                        and if the word is in the model library, added to the total\n",
    "        - output: each review is represented as an average feature vector \n",
    "    \"\"\"\n",
    "    \n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. \n",
    "    # Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "\n",
    "    # Loop over each word in the review and, if it is in the model's vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            # vector sum of each word vector\n",
    "            featureVec = np.add(featureVec,model[word]) # np.add: element-wise summation \n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords) # np.divide: element-wise division\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), \n",
    "    # calculate the average feature vector for each one and return a 2D numpy array \n",
    "\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "    # Print a status message every 1000th review\n",
    "        if counter%5000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    " \n",
    "    # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        \n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load review_to_wordlist.py\n",
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "\n",
    "    # Function to convert a document to a sequence of words, optionally removing stop words.  Returns a list of words.\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"data/kaggle_nlp_labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv( \"data/kaggle_nlp_testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv( \"data/kaggle_nlp_unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:47: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. \n",
    "\n",
    "# Notice that we now use stop word removal.\n",
    "clean_train_reviews = []\n",
    "num_features = 300\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print \"Creating average feature vecs for test reviews\"\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model using the average vectors (random forest)\n",
    "We can only use the labeled training reviews to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "# output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that this produced results much better than chance, but underperformed Bag of Words by a few percentage points. Since the element-wise average of the vectors didn't produce spectacular results, perhaps we could do it in a more intelligent way? \n",
    "\n",
    "A standard way of weighting word vectors is to apply **tf-idf** weights, which measure **how important a given word is within a given set of documents**. \n",
    "- One way to extract tf-idf weights in Python is by using scikit-learn's **`TfidfVectorizer`**, which has an interface similar to the CountVectorizer that we used in Part 1. \n",
    "- However, when we tried weighting our word vectors in this way, we found no substantial improvement in performance.\n",
    "\n",
    "# 3. From Words to Paragraphs, Attempt 2: Clustering\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to **exploit the similarity of words within a cluster**. Grouping vectors in this way is known as **vector quantization**. To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means.\n",
    "\n",
    "## K-Means clustering (*long run time because of high K*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  1824.00886083 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For convenience, we zip these into one dictionary as follows:\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "[u'tired', u'bored', u'scared']\n",
      "\n",
      "Cluster 1\n",
      "[u'unexpectedly', u'engages', u'decidedly', u'unassuming', u'stable']\n",
      "\n",
      "Cluster 2\n",
      "[u'artifact', u'lowly', u'researching', u'detour', u'pact', u'conducting', u'dong', u'mp', u'interviewing', u'abbey', u'experimenting']\n",
      "\n",
      "Cluster 3\n",
      "[u'thespian', u'gutsy', u'meaty', u'asset']\n",
      "\n",
      "Cluster 4\n",
      "[u'tanner', u'valdez']\n",
      "\n",
      "Cluster 5\n",
      "[u'regional', u'specifically', u'descent', u'traditionally']\n",
      "\n",
      "Cluster 6\n",
      "[u'revisit', u'sneakers', u'skipping', u'tivo', u'heres', u'queue', u'nay', u'cough', u'til', u'counting']\n",
      "\n",
      "Cluster 7\n",
      "[u'illustrate', u'myriad']\n",
      "\n",
      "Cluster 8\n",
      "[u'gist', u'summarized', u'paragraph', u'condensed']\n",
      "\n",
      "Cluster 9\n",
      "[u'claws', u'maggots', u'swords', u'organs', u'chickens', u'tattoos', u'knives', u'bats']\n"
     ]
    }
   ],
   "source": [
    "# Check words in the first 10 clusters\n",
    "for cluster in xrange(0,10):\n",
    "    # Print the cluster number  \n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    \n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of \"centroids\"\n",
    "Now we have a cluster (or \"centroid\") assignment for each word. \n",
    "- We can define a function to convert reviews into bags-of-centroids. \n",
    "- This works just like Bag of Words but uses semantically related clusters instead of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    \n",
    "    # The number of clusters is equal to the highest cluster index in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    \n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    \n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting (random forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "# output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
