{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Workflows in Python</font>\n",
    "This is to follow the example code from [Katie Malone's blog post](https://civisanalytics.com/blog/data-science/2015/12/17/workflows-in-python-getting-data-ready-to-build-models/) at Civis Analytics. This gives an example of a workflow model for Python. She describes as \n",
    "\n",
    "**\"a workflow that focuses on getting a quick-and-dirty model up and running as quickly as possible, and then going back to iterate on the weak points until the model seems to be converging on an answer.\"**\n",
    "\n",
    "- Dataset: “Pump it Up: Mining the Water Table” challenge on drivendata.org, which has examples of wells in Africa, their characteristics and whether they are **functional, non-functional, or functional but in need of repair.** \n",
    "\n",
    "-  Goal: build a model that will take the characteristics of a well and predict correctly which category that well falls into.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <font color = \"blue\"> [Part 1: Getting Data Ready to Build Models](https://civisanalytics.com/blog/data-science/2015/12/17/workflows-in-python-getting-data-ready-to-build-models/) </font>\n",
    "\n",
    "# Getting started\n",
    "- read in data\n",
    "- transform features and labels to make the data amenable to machine learning\n",
    "- pick a modeling strategy (classification)\n",
    "- make a train/test split (this was done implicitly when I called cross_val_score)\n",
    "- evaluate several models for identifying wells that are failed or in danger of failing\n",
    "\n",
    "# 1. Labels\n",
    "## A quick print statement on the labels shows that the labels are strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  status_group\n",
      "id                            \n",
      "69572               functional\n",
      "8776                functional\n",
      "34310               functional\n",
      "67743           non functional\n",
      "19728               functional\n",
      "9944                functional\n",
      "19816           non functional\n",
      "54551           non functional\n",
      "53934           non functional\n",
      "46144               functional\n",
      "49056               functional\n",
      "50409               functional\n",
      "36957               functional\n",
      "50495               functional\n",
      "53752               functional\n",
      "61848               functional\n",
      "48451           non functional\n",
      "58155           non functional\n",
      "34169  functional needs repair\n",
      "18274               functional\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "features_df = pd.DataFrame.from_csv(\"data/training_set_values.csv\")\n",
    "labels_df   = pd.DataFrame.from_csv(\"data/training_set_labels.csv\") \n",
    "print(labels_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping labels to integers\n",
    "\"When I want a specific mapping between strings and integers, like here, doing it manually is usually the way I go.\"\n",
    "- there’s also the sklearn LabelEncoder.\n",
    "- pandas applymap()\n",
    "    - apply() vs. applymap(): applymap() operates on a whole dataframe while apply() operates on a series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas applymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       status_group\n",
      "id                 \n",
      "69572             2\n",
      "8776              2\n",
      "34310             2\n",
      "67743             0\n",
      "19728             2\n"
     ]
    }
   ],
   "source": [
    "def label_map(y):\n",
    "    if y==\"functional\":\n",
    "        return 2\n",
    "    elif y==\"functional needs repair\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "labels_df = labels_df.applymap(label_map)\n",
    "print(labels_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       amount_tsh date_recorded        funder  gps_height     installer  \\\n",
      "id                                                                        \n",
      "69572        6000    2011-03-14         Roman        1390         Roman   \n",
      "8776            0    2013-03-06       Grumeti        1399       GRUMETI   \n",
      "34310          25    2013-02-25  Lottery Club         686  World vision   \n",
      "67743           0    2013-01-28        Unicef         263        UNICEF   \n",
      "19728           0    2011-07-13   Action In A           0       Artisan   \n",
      "\n",
      "       longitude   latitude              wpt_name  num_private  \\\n",
      "id                                                               \n",
      "69572  34.938093  -9.856322                  none            0   \n",
      "8776   34.698766  -2.147466              Zahanati            0   \n",
      "34310  37.460664  -3.821329           Kwa Mahundi            0   \n",
      "67743  38.486161 -11.155298  Zahanati Ya Nanyumbu            0   \n",
      "19728  31.130847  -1.825359               Shuleni            0   \n",
      "\n",
      "                         basin          ...          payment_type  \\\n",
      "id                                      ...                         \n",
      "69572               Lake Nyasa          ...              annually   \n",
      "8776             Lake Victoria          ...             never pay   \n",
      "34310                  Pangani          ...            per bucket   \n",
      "67743  Ruvuma / Southern Coast          ...             never pay   \n",
      "19728            Lake Victoria          ...             never pay   \n",
      "\n",
      "      water_quality  quality_group      quantity quantity_group  \\\n",
      "id                                                                \n",
      "69572          soft           good        enough         enough   \n",
      "8776           soft           good  insufficient   insufficient   \n",
      "34310          soft           good        enough         enough   \n",
      "67743          soft           good           dry            dry   \n",
      "19728          soft           good      seasonal       seasonal   \n",
      "\n",
      "                     source           source_type source_class  \\\n",
      "id                                                               \n",
      "69572                spring                spring  groundwater   \n",
      "8776   rainwater harvesting  rainwater harvesting      surface   \n",
      "34310                   dam                   dam      surface   \n",
      "67743           machine dbh              borehole  groundwater   \n",
      "19728  rainwater harvesting  rainwater harvesting      surface   \n",
      "\n",
      "                   waterpoint_type waterpoint_type_group  \n",
      "id                                                        \n",
      "69572           communal standpipe    communal standpipe  \n",
      "8776            communal standpipe    communal standpipe  \n",
      "34310  communal standpipe multiple    communal standpipe  \n",
      "67743  communal standpipe multiple    communal standpipe  \n",
      "19728           communal standpipe    communal standpipe  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many of the features are categorical and they need to be transformed to numerical values.\n",
    "- transform categorical features: OneHotEncoder in sklearn or get_dummies() in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_feature( df, column_name ):\n",
    "    \n",
    "    \"\"\" take features_df and the name of a column in that dataframe, \n",
    "        and return the same dataframe but \n",
    "        with the indicated feature encoded with integers rather than strings\"\"\"\n",
    "    \n",
    "    unique_values = set( df[column_name].tolist() )\n",
    "    transformer_dict = {}\n",
    "    for ii, value in enumerate(unique_values):\n",
    "        transformer_dict[value] = ii\n",
    "\n",
    "    def label_map(y):\n",
    "        return transformer_dict[y]\n",
    "    df[column_name] = df[column_name].apply( label_map )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       amount_tsh date_recorded  funder  gps_height  installer  longitude  \\\n",
      "id                                                                          \n",
      "69572        6000    2011-03-14    1539        1390       1749  34.938093   \n",
      "8776            0    2013-03-06     774        1399        136  34.698766   \n",
      "34310          25    2013-02-25     906         686       1400  37.460664   \n",
      "67743           0    2013-01-28     590         263        556  38.486161   \n",
      "19728           0    2011-07-13    1341           0        537  31.130847   \n",
      "\n",
      "        latitude  wpt_name  num_private  basin          ...            \\\n",
      "id                                                      ...             \n",
      "69572  -9.856322     15203            0      5          ...             \n",
      "8776   -2.147466      5611            0      7          ...             \n",
      "34310  -3.821329      6138            0      6          ...             \n",
      "67743 -11.155298      1252            0      1          ...             \n",
      "19728  -1.825359     29682            0      7          ...             \n",
      "\n",
      "       payment_type  water_quality  quality_group  quantity  quantity_group  \\\n",
      "id                                                                            \n",
      "69572             4              0              0         2               2   \n",
      "8776              5              0              0         1               1   \n",
      "34310             1              0              0         2               2   \n",
      "67743             5              0              0         0               0   \n",
      "19728             5              0              0         3               3   \n",
      "\n",
      "       source  source_type  source_class  waterpoint_type  \\\n",
      "id                                                          \n",
      "69572       1            1             1                1   \n",
      "8776        6            4             2                1   \n",
      "34310       7            5             2                4   \n",
      "67743       2            6             1                4   \n",
      "19728       6            4             2                1   \n",
      "\n",
      "       waterpoint_type_group  \n",
      "id                            \n",
      "69572                      1  \n",
      "8776                       1  \n",
      "34310                      1  \n",
      "67743                      1  \n",
      "19728                      1  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "['amount_tsh' 'funder' 'gps_height' 'installer' 'longitude' 'latitude'\n",
      " 'wpt_name' 'num_private' 'basin' 'subvillage' 'region' 'region_code'\n",
      " 'district_code' 'lga' 'ward' 'population' 'public_meeting' 'recorded_by'\n",
      " 'scheme_management' 'scheme_name' 'permit' 'construction_year'\n",
      " 'extraction_type' 'extraction_type_group' 'extraction_type_class'\n",
      " 'management' 'management_group' 'payment' 'payment_type' 'water_quality'\n",
      " 'quality_group' 'quantity' 'quantity_group' 'source' 'source_type'\n",
      " 'source_class' 'waterpoint_type' 'waterpoint_type_group']\n"
     ]
    }
   ],
   "source": [
    "### list of column names indicating which columns to transform; \n",
    "### this is just a start!  Use some of the print( labels_df.head() )\n",
    "### output upstream to help you decide which columns get the\n",
    "### transformation\n",
    "\n",
    "names_of_columns_to_transform = [\"funder\", \"installer\", \"wpt_name\", \"basin\", \"subvillage\",\n",
    "                    \"region\", \"lga\", \"ward\", \"public_meeting\", \"recorded_by\",\n",
    "                    \"scheme_management\", \"scheme_name\", \"permit\",\n",
    "                    \"extraction_type\", \"extraction_type_group\",\n",
    "                    \"extraction_type_class\",\n",
    "                    \"management\", \"management_group\",\n",
    "                    \"payment\", \"payment_type\",\n",
    "                    \"water_quality\", \"quality_group\", \"quantity\", \"quantity_group\",\n",
    "                    \"source\", \"source_type\", \"source_class\",\n",
    "                    \"waterpoint_type\", \"waterpoint_type_group\"]\n",
    "\n",
    "for column in names_of_columns_to_transform:\n",
    "    features_df = transform_feature( features_df, column )\n",
    "    \n",
    "print( features_df.head() )\n",
    "    \n",
    "### remove the \"date_recorded\" column--we're not going to make use\n",
    "### of time-series data today\n",
    "features_df.drop(\"date_recorded\", axis=1, inplace=True)\n",
    "\n",
    "print(features_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep for sklearn: convert it to numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = features_df.as_matrix()\n",
    "y = labels_df[\"status_group\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train and test \n",
    "- The cheapest and easiest way to train on one portion of my dataset and test on another, and to get a measure of model quality at the same time, is to use **sklearn.cross_validation.cross_val_score()**\n",
    "    - Splits my data into three equal portions, trains on two of them, and tests on the third\n",
    "    - This process repeats three times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.68363636  0.6830303   0.6859596 ]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.cross_validation\n",
    "clf = sklearn.linear_model.LogisticRegression()\n",
    "score = sklearn.cross_validation.cross_val_score( clf, X, y )\n",
    "print( score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification or Regression\n",
    "- I have the choice of **modeling with a classifier** and potentially getting slightly worse performance, \n",
    "- or building **a regression but needing to add a post-processing step that turns my continuous (i.e. float) predictions into integer category labels.** \n",
    "- I’ve decided to go with the classification approach for this example, but this is a decision made for convenience that I could revisit when improving my model down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compare algorithms\n",
    "- I started with a simple logistic regression above (despite the name, this is a classification algorithm) \n",
    "- I’ll compare to a couple of other classifiers, a decision tree classifier and a random forest classifier, to see which one seems to do the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73959596  0.73580808  0.73020202]\n",
      "[ 0.78868687  0.78929293  0.78893939]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier()\n",
    "score = sklearn.cross_validation.cross_val_score( clf, X, y )\n",
    "print( score )\n",
    "\n",
    "clf = sklearn.ensemble.RandomForestClassifier()\n",
    "score = sklearn.cross_validation.cross_val_score( clf, X, y )\n",
    "print( score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# <font color = \"blue\"> [Part 2: Curating Features and Thinking Scientifically about Algorithms](https://civisanalytics.com/blog/data-science/2015/12/23/workflows-in-python-curating-features-and-thinking-scientifically-about-algorithms/) </font>\n",
    "\n",
    "# 1. Revisiting numerical encoding of categorical features\n",
    "\n",
    "- A problem with representing categorical variables as integers is that **integers are ordered**, while categories are not. \n",
    "- The standard way to deal with this is to use **dummy variables**; **one-hot encoding** is a very common way of dummying. \n",
    "\n",
    "## One-hot encoding\n",
    "- The categories are expanded over several boolean columns, only one of which is true (hot). \n",
    "- the scikit-learn `OneHotEncoder` object or pandas `get_dummies()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "def hot_encoder(df, column_name):\n",
    "    \n",
    "    \"\"\"\n",
    "        a one-hot-encoder function \n",
    "        that takes the data frame and the title of a column, and \n",
    "        returns the same data frame but one-hot encoding performed on the indicated feature\n",
    "    \"\"\"\n",
    "    \n",
    "    column = df[column_name].tolist()\n",
    "    column = np.reshape( column, (len(column), 1) )  ### needs to be an N x 1 numpy array\n",
    "    enc = sklearn.preprocessing.OneHotEncoder()\n",
    "    enc.fit( column )\n",
    "    new_column = enc.transform( column ).toarray()\n",
    "    column_titles = []\n",
    "    \n",
    "    ### making titles for the new columns, and appending them to dataframe\n",
    "    for ii in range( len(new_column[0]) ):\n",
    "        this_column_name = column_name+\"_\"+str(ii)\n",
    "        df[this_column_name] = new_column[:,ii]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding makes the dataset bigger–sometimes a lot bigger. \n",
    "You can imagine that this can sometimes get really, really big (imagine a column encoding all the counties in the United States, for example).\n",
    "- There are some columns in this example that will really blow up the dataset, so I’ll remove them before proceeding with the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount_tsh' 'funder' 'gps_height' 'installer' 'longitude' 'latitude'\n",
      " 'wpt_name' 'num_private' 'basin' 'subvillage' 'region' 'region_code'\n",
      " 'district_code' 'lga' 'ward' 'population' 'public_meeting' 'recorded_by'\n",
      " 'scheme_management' 'scheme_name' 'permit' 'construction_year'\n",
      " 'extraction_type' 'extraction_type_group' 'extraction_type_class'\n",
      " 'management' 'management_group' 'payment' 'payment_type' 'water_quality'\n",
      " 'quality_group' 'quantity' 'quantity_group' 'source' 'source_type'\n",
      " 'source_class' 'waterpoint_type' 'waterpoint_type_group']\n"
     ]
    }
   ],
   "source": [
    "print(features_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df.drop( \"funder\", axis=1, inplace=True )\n",
    "features_df.drop( \"installer\", axis=1, inplace=True )\n",
    "features_df.drop( \"wpt_name\", axis=1, inplace=True )\n",
    "features_df.drop( \"subvillage\", axis=1, inplace=True )\n",
    "features_df.drop( \"ward\", axis=1, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_of_columns_to_transform.remove(\"funder\")\n",
    "names_of_columns_to_transform.remove(\"installer\")\n",
    "names_of_columns_to_transform.remove(\"wpt_name\")\n",
    "names_of_columns_to_transform.remove(\"subvillage\")\n",
    "names_of_columns_to_transform.remove(\"ward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       amount_tsh  gps_height  longitude   latitude  num_private  basin  \\\n",
      "id                                                                        \n",
      "69572        6000        1390  34.938093  -9.856322            0      5   \n",
      "8776            0        1399  34.698766  -2.147466            0      7   \n",
      "34310          25         686  37.460664  -3.821329            0      6   \n",
      "67743           0         263  38.486161 -11.155298            0      1   \n",
      "19728           0           0  31.130847  -1.825359            0      7   \n",
      "\n",
      "       region  region_code  district_code  lga           ...             \\\n",
      "id                                                       ...              \n",
      "69572       7           11              5   65           ...              \n",
      "8776        3           20              2   86           ...              \n",
      "34310       4           21              4   11           ...              \n",
      "67743      20           90             63  122           ...              \n",
      "19728       1           18              1   70           ...              \n",
      "\n",
      "       waterpoint_type_3  waterpoint_type_4  waterpoint_type_5  \\\n",
      "id                                                               \n",
      "69572                  0                  0                  0   \n",
      "8776                   0                  0                  0   \n",
      "34310                  0                  1                  0   \n",
      "67743                  0                  1                  0   \n",
      "19728                  0                  0                  0   \n",
      "\n",
      "       waterpoint_type_6  waterpoint_type_group_0  waterpoint_type_group_1  \\\n",
      "id                                                                           \n",
      "69572                  0                        0                        1   \n",
      "8776                   0                        0                        1   \n",
      "34310                  0                        0                        1   \n",
      "67743                  0                        0                        1   \n",
      "19728                  0                        0                        1   \n",
      "\n",
      "       waterpoint_type_group_2  waterpoint_type_group_3  \\\n",
      "id                                                        \n",
      "69572                        0                        0   \n",
      "8776                         0                        0   \n",
      "34310                        0                        0   \n",
      "67743                        0                        0   \n",
      "19728                        0                        0   \n",
      "\n",
      "       waterpoint_type_group_4  waterpoint_type_group_5  \n",
      "id                                                       \n",
      "69572                        0                        0  \n",
      "8776                         0                        0  \n",
      "34310                        0                        0  \n",
      "67743                        0                        0  \n",
      "19728                        0                        0  \n",
      "\n",
      "[5 rows x 3031 columns]\n"
     ]
    }
   ],
   "source": [
    "for feature in names_of_columns_to_transform:\n",
    "    features_df = hot_encoder( features_df, feature )\n",
    "\n",
    "print( features_df.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3000 features (origianlly we had about 40). \n",
    "\n",
    "# 2. Feature selection\n",
    "Having so many features invites problems with overfitting, slow and memory-intensive training.\n",
    "- This is a perfect use case for feature selection, which is supported in scikit-learn by e.g. `SelectKBest()`, which will do **univariate feature selection** to get the k features (where k is a number which I have to tell the algorithm). \n",
    "- Making a guess, I can ask for the top 100 features, which doesn’t make my performance much worse and speeds things up a lot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_selection\n",
    "\n",
    "select = sklearn.feature_selection.SelectKBest(k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:111: UserWarning: Features [ 12 191] are constant.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "X = features_df.as_matrix()\n",
    "selected_X = select.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Back to Machine Learning\n",
    "In the last post, a **random forest classifier did the best** of all the models I tried, beating a logistic regression (by a lot) and a decision tree classifier (by a slimmer margin). \n",
    "\n",
    "Here are a few reasons why:\n",
    "\n",
    "## Logistic regression\n",
    "- A logistic regression is **an example of a linear model**, which (unless you make special adaptations, which I’ll detail in a moment) assumes that **the relationship between each of my features and the output class is a linear one**. For example, if one of the features is the depth of a well, a linear model will assume that (all other things being equal) the difference in functionality between a 20-foot-deep well and a 40-foot-deep one will be the same as the difference between 40 feet and 60 feet. This isn’t always a valid assumption. \n",
    "    - **One way to address it is to add extra features** like depth squared and the logarithm of depth, which helps a linear model capture nonlinearities, but might not still allow me to get all the nuances of nonlinear relationships.\n",
    "\n",
    "- A logistic regression also **doesn’t capture interactions between features**, for example that deep wells might be largely functional and wells drilled in rock are largely functional but deep wells in rocky places are largely non-functional. \n",
    "    - Again, I can explicitly add interaction terms to the logistic regression, but this **gets unwieldy fast when I have many features.**\n",
    "\n",
    "\n",
    "## Decision Tree\n",
    "- A decision tree **can capture interactions and nonlinearities much more naturally than logistic regression**, because of the binary tree structure of the decision tree algorithm itself. \n",
    "    - The downside of decision trees is that **they can be harder to interpret or assign uncertainties to their predictions.**\n",
    "\n",
    "## Random Forest\n",
    "- A random forest is a collection of decision trees, each of which is trained on **a subset of the rows/columns** of the training data. The randomness in the training set means that the individual trees in a random forest are high-variance, but low-bias, and the final prediction is made by **having each tree classify a given event and then using their predictions as “votes,” with the majority opinion being assigned as the label**. \n",
    "- I have the nonlinearities and interactions being **captured by the individual trees**, but ensembling many trees into a random forest tends to **cancel out the biases/shortcomings of any one tree** and I get a stronger predictor overall.\n",
    "- In [empirical studies](https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf) of many algorithms being applied to many **supervised learning problems**, random forests often come out on top overall. So **when in doubt, or if I only have the time/resources to try one model**, a random forest is likely to get at or near the peak performance of all the algorithms on the market.\n",
    "- If it was tricky to interpret or compute errors for a decision tree, a random forest is only going to be worse because there’s now 50-100 decision trees to worry about.\n",
    "\n",
    "## Random forests have lots of parameters to optimize. \n",
    "- How many trees should there be? \n",
    "- How does each tree get trained? \n",
    "- How many features get used in training each tree? \n",
    "\n",
    "There usually aren’t formulaic answers to these questions, and part of the craft of machine learning is **tuning these parameters to get the best performance** that I can out of my model. \n",
    "- But with so many parameters, which sometimes interact with each other in complex ways, parameter tuning can be a huge hassle. \n",
    "- In the next post, I’ll talk about an extremely powerful pair of tools in scikit-learn, the Pipeline and GridSearchCV, that allow crazy powerful parameter tuning in just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Part 3: Using Pipeline and GridSearchCV for More Compact and Comprehensive Code](https://civisanalytics.com/blog/data-science/2016/01/06/workflows-python-using-pipeline-gridsearchcv-for-compact-code/)\n",
    "\n",
    "- Now we're left with **a lot of free parameters of the algorithms to tune**, and messing around with the workflow often leads to **spaghetti code** that becomes less and less understandable/easy to experiment with as we go.\n",
    "- Enter the scikit-learn Pipeline and GridSearchCV objects: two tools that effectively allow us to pour gasoline on  data science fire, tightening up the code and doing parameter scans in just a few lines of code.\n",
    "\n",
    "# 1. Pipeline\n",
    "- There are a number of tools that I’ve chained together to get where I am now, like SelectKBest and RandomForestClassifier. After selecting the 100 best features, **the natural next step is to run my random forest again to see if it does a little better with fewer features.** In this case, I have SelectKBest doing selection, with the output of that process going straight into a classifier. \n",
    "- **Pipeline packages the transformation step of SelectKBest with the estimation step of RandomForestClassifier into a coherent workflow.**\n",
    "\n",
    "\n",
    "## Benefit\n",
    "- It makes code more **readable** (or, if you like, it makes the intent of the code clearer).\n",
    "- I don’t have to worry about keeping track data during intermediate steps, for example between transforming and estimating.\n",
    "- It makes it trivial to move ordering of the pipeline pieces, or to swap pieces in and out.\n",
    "- **It allows you to do `GridSearchCV` on your workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.77      0.77      7458\n",
      "          1       0.42      0.36      0.39      1425\n",
      "          2       0.80      0.81      0.81     10719\n",
      "\n",
      "avg / total       0.76      0.76      0.76     19602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:111: UserWarning: Features [  12  191  206  211  232  238  273  278  287  291  302  303  307  313  322\n",
      "  324  325  344  353  359  371  378  383  423  429  432  433  437  440  444\n",
      "  450  453  479  481  496  509  523  529  533  540  542  546  549  561  566\n",
      "  597  609  620  648  649  656  684  700  727  735  748  751  761  782  797\n",
      "  810  812  816  826  829  836  845  847  853  855  856  867  889  894  903\n",
      "  905  912  944  946  965  975  977 1005 1024 1026 1034 1036 1048 1051 1052\n",
      " 1053 1070 1077 1096 1106 1118 1120 1139 1145 1146 1157 1158 1161 1172 1174\n",
      " 1180 1188 1191 1197 1200 1203 1205 1242 1244 1247 1256 1262 1277 1283 1286\n",
      " 1287 1290 1293 1294 1298 1307 1312 1315 1326 1335 1362 1366 1374 1398 1399\n",
      " 1402 1405 1418 1423 1425 1426 1429 1432 1441 1456 1482 1494 1500 1511 1522\n",
      " 1530 1537 1545 1560 1561 1563 1566 1586 1606 1613 1620 1628 1629 1637 1638\n",
      " 1640 1645 1653 1676 1688 1707 1715 1717 1719 1720 1722 1725 1726 1735 1739\n",
      " 1740 1754 1758 1762 1799 1818 1825 1835 1838 1845 1852 1866 1873 1874 1880\n",
      " 1883 1887 1899 1910 1915 1922 1931 1945 1947 1960 1968 1969 1992 1994 2009\n",
      " 2010 2023 2027 2039 2042 2049 2066 2074 2081 2109 2122 2124 2128 2130 2139\n",
      " 2162 2165 2170 2196 2197 2212 2217 2220 2239 2262 2291 2305 2307 2322 2327\n",
      " 2336 2347 2354 2373 2375 2408 2417 2434 2436 2456 2466 2476 2478 2479 2485\n",
      " 2490 2501 2504 2506 2525 2526 2532 2539 2540 2548 2559 2579 2590 2596 2597\n",
      " 2608 2620 2629 2633 2634 2640 2659 2671 2675 2679 2686 2688 2693 2699 2702\n",
      " 2703 2723 2729 2738 2745 2753 2755 2763 2767 2772 2775 2784 2786 2795 2825\n",
      " 2826 2827 2841 2880 2900] are constant.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.pipeline\n",
    "\n",
    "select = sklearn.feature_selection.SelectKBest(k=100)\n",
    "clf = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "steps = [('feature_selection', select), ('random_forest', clf)]\n",
    "\n",
    "pipeline = sklearn.pipeline.Pipeline(steps)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "### fit your pipeline on X_train and y_train\n",
    "pipeline.fit( X_train, y_train )\n",
    "\n",
    "### call pipeline.predict() on your X_test data to make a set of test predictions\n",
    "y_prediction = pipeline.predict( X_test )\n",
    "\n",
    "### test your predictions using sklearn.classification_report()\n",
    "### classification_report gives more info than cross_val_score\n",
    "### But we need to split train/test separately (with cross_val_score, you don't have to)\n",
    "report = sklearn.metrics.classification_report( y_test, y_prediction )\n",
    "\n",
    "### and print the report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GridSearchCV\n",
    "\n",
    "When I decided to select the 100 best features, setting that number to 100 was kind of a hand-wavey decision. Similarly, the RandomForestClassifier that I’m using right now has all its parameters set to their default values, which might not be optimal.\n",
    "\n",
    "So, a straightforward thing to do now is **to try different values of k** (the number of features being used in the model) and **any RandomForestClassifier parameters I want to tune** (for the sake of concreteness, I’ll play with n_estimators and min_samples_split).\n",
    "\n",
    "- Trying lots of values for each of these free parameters is tedious\n",
    "- There can sometimes be interactions between the choices I make in one step and the optimal value for a downstream step. \n",
    "- To avoid local optima, I should try all the combinations of parameters, and not just vary them independently. If I want to try 5 different values each for k, n_estimators and min_samples_split, that means 5 x 5 x 5 = 125 different combinations to try. Not something I want to do by hand.\n",
    "\n",
    "`GridSearchCV` allows me to construct a grid of all the combinations of parameters, tries each combination, and then reports back the best combination/model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:111: UserWarning: Features [  12  191  193  206  211  229  232  238  256  269  270  273  277  278  287\n",
      "  290  291  302  303  307  310  313  315  322  324  325  335  344  347  353\n",
      "  359  364  371  378  383  392  403  410  419  422  423  429  432  433  437\n",
      "  440  444  450  453  459  462  474  479  481  485  490  496  502  504  507\n",
      "  509  521  523  529  533  535  537  538  539  540  541  542  546  547  549\n",
      "  551  558  561  566  570  583  585  593  597  602  609  620  621  639  647\n",
      "  648  649  656  677  683  684  691  693  700  705  727  734  735  738  748\n",
      "  749  751  755  761  767  777  781  782  793  795  797  801  810  812  816\n",
      "  822  825  826  829  836  845  847  853  855  856  862  863  867  870  873\n",
      "  889  890  891  894  903  905  910  912  940  944  946  947  950  961  965\n",
      "  975  977  990  994 1004 1005 1011 1012 1024 1026 1031 1034 1036 1044 1048\n",
      " 1051 1052 1053 1062 1070 1077 1091 1096 1097 1099 1100 1104 1105 1106 1107\n",
      " 1118 1120 1122 1132 1137 1139 1145 1146 1152 1154 1157 1158 1161 1171 1172\n",
      " 1174 1180 1186 1188 1191 1197 1200 1203 1205 1212 1225 1228 1238 1240 1242\n",
      " 1244 1247 1248 1252 1256 1257 1258 1262 1266 1270 1277 1281 1283 1286 1287\n",
      " 1290 1293 1294 1298 1307 1312 1315 1317 1321 1326 1328 1335 1336 1339 1353\n",
      " 1362 1366 1374 1377 1379 1382 1384 1386 1394 1398 1399 1400 1402 1405 1417\n",
      " 1418 1422 1423 1425 1426 1429 1432 1436 1437 1439 1441 1456 1461 1466 1475\n",
      " 1482 1484 1486 1494 1496 1498 1500 1511 1513 1522 1525 1526 1527 1529 1530\n",
      " 1536 1537 1545 1560 1561 1563 1566 1567 1568 1579 1585 1586 1606 1613 1620\n",
      " 1622 1623 1626 1628 1629 1636 1637 1638 1639 1640 1645 1653 1659 1667 1676\n",
      " 1688 1694 1697 1699 1707 1709 1715 1717 1719 1720 1721 1722 1725 1726 1735\n",
      " 1739 1740 1745 1754 1758 1759 1762 1774 1798 1799 1812 1816 1818 1825 1828\n",
      " 1833 1835 1837 1838 1845 1848 1851 1852 1859 1866 1873 1874 1880 1883 1886\n",
      " 1887 1893 1899 1910 1915 1922 1923 1931 1941 1945 1947 1960 1968 1969 1976\n",
      " 1984 1992 1993 1994 2009 2010 2012 2023 2027 2039 2042 2049 2066 2072 2074\n",
      " 2076 2081 2095 2107 2109 2122 2124 2128 2130 2139 2141 2144 2152 2160 2162\n",
      " 2165 2170 2176 2185 2193 2196 2197 2203 2212 2217 2220 2232 2239 2255 2262\n",
      " 2277 2289 2291 2293 2295 2296 2305 2307 2319 2322 2327 2328 2332 2335 2336\n",
      " 2347 2348 2354 2357 2358 2373 2375 2383 2385 2389 2397 2402 2406 2407 2408\n",
      " 2417 2428 2434 2436 2441 2454 2455 2456 2460 2464 2466 2471 2476 2478 2479\n",
      " 2481 2485 2490 2494 2495 2497 2499 2501 2502 2504 2506 2521 2525 2526 2527\n",
      " 2529 2532 2534 2539 2540 2544 2546 2547 2548 2552 2559 2571 2572 2579 2584\n",
      " 2588 2590 2596 2597 2608 2620 2629 2632 2633 2634 2635 2637 2640 2650 2654\n",
      " 2659 2661 2671 2672 2675 2679 2686 2688 2693 2695 2699 2700 2702 2703 2705\n",
      " 2710 2712 2719 2723 2726 2728 2729 2730 2735 2738 2741 2745 2747 2753 2755\n",
      " 2763 2767 2772 2775 2783 2784 2786 2795 2807 2812 2813 2821 2823 2825 2826\n",
      " 2827 2838 2841 2843 2849 2852 2871 2880 2900] are constant.\n",
      "  UserWarning)\n",
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:111: UserWarning: Features [  12  191  206  211  215  220  222  224  232  235  238  242  250  257  262\n",
      "  263  266  273  278  287  291  298  302  303  307  312  313  322  324  325\n",
      "  328  330  339  344  345  353  359  360  369  371  374  377  378  383  387\n",
      "  390  395  408  413  423  424  428  429  432  433  437  440  442  444  450\n",
      "  451  453  475  479  481  488  496  509  523  529  533  540  542  546  549\n",
      "  561  566  574  591  597  608  609  610  617  620  623  637  648  649  656\n",
      "  674  676  680  684  688  692  694  700  702  717  727  728  730  733  735\n",
      "  746  748  751  759  761  770  773  774  782  797  800  803  806  807  810\n",
      "  812  816  826  828  829  836  845  847  853  854  855  856  867  889  893\n",
      "  894  903  905  912  913  914  918  928  944  946  957  965  975  977  996\n",
      "  998  999 1005 1006 1022 1024 1025 1026 1034 1036 1037 1048 1051 1052 1053\n",
      " 1054 1055 1057 1060 1061 1070 1074 1077 1081 1092 1096 1106 1110 1111 1112\n",
      " 1118 1120 1127 1128 1136 1138 1139 1145 1146 1157 1158 1161 1162 1172 1174\n",
      " 1177 1180 1181 1188 1191 1197 1200 1202 1203 1205 1210 1229 1230 1242 1244\n",
      " 1247 1254 1256 1262 1277 1283 1286 1287 1290 1293 1294 1298 1307 1310 1312\n",
      " 1315 1326 1334 1335 1337 1338 1347 1351 1354 1362 1366 1373 1374 1390 1391\n",
      " 1398 1399 1402 1405 1418 1423 1425 1426 1429 1432 1438 1441 1455 1456 1465\n",
      " 1467 1473 1482 1485 1494 1499 1500 1511 1514 1522 1530 1534 1537 1543 1545\n",
      " 1547 1548 1549 1556 1560 1561 1563 1564 1566 1571 1573 1576 1586 1594 1595\n",
      " 1597 1605 1606 1607 1613 1614 1617 1619 1620 1628 1629 1632 1637 1638 1640\n",
      " 1645 1649 1653 1669 1672 1676 1686 1688 1700 1707 1715 1717 1719 1720 1722\n",
      " 1725 1726 1734 1735 1739 1740 1741 1752 1754 1758 1762 1763 1767 1768 1771\n",
      " 1788 1789 1795 1799 1808 1818 1819 1823 1825 1826 1835 1838 1845 1847 1852\n",
      " 1863 1866 1868 1871 1873 1874 1880 1881 1882 1883 1887 1889 1890 1899 1907\n",
      " 1910 1915 1916 1918 1919 1922 1931 1932 1938 1945 1947 1955 1960 1968 1969\n",
      " 1985 1992 1994 2002 2004 2009 2010 2017 2018 2020 2022 2023 2027 2039 2042\n",
      " 2044 2049 2050 2061 2066 2069 2074 2078 2081 2109 2111 2122 2124 2128 2130\n",
      " 2131 2139 2140 2153 2155 2162 2165 2170 2171 2172 2181 2182 2187 2196 2197\n",
      " 2200 2212 2217 2218 2219 2220 2227 2233 2235 2239 2244 2245 2248 2260 2262\n",
      " 2275 2278 2283 2290 2291 2294 2300 2304 2305 2307 2322 2327 2329 2336 2337\n",
      " 2347 2354 2365 2373 2375 2377 2386 2393 2408 2417 2434 2436 2437 2440 2442\n",
      " 2456 2461 2462 2465 2466 2476 2478 2479 2485 2487 2490 2501 2504 2506 2510\n",
      " 2511 2519 2525 2526 2532 2539 2540 2543 2548 2557 2559 2575 2579 2583 2590\n",
      " 2596 2597 2600 2608 2620 2624 2629 2633 2634 2640 2643 2645 2651 2655 2659\n",
      " 2662 2667 2671 2675 2679 2681 2685 2686 2688 2693 2698 2699 2702 2703 2717\n",
      " 2718 2723 2729 2738 2745 2753 2755 2763 2765 2767 2772 2775 2779 2780 2784\n",
      " 2786 2789 2790 2795 2796 2805 2825 2826 2827 2828 2835 2841 2847 2856 2861\n",
      " 2869 2873 2875 2880 2882 2900] are constant.\n",
      "  UserWarning)\n",
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:111: UserWarning: Features [  12  104  191  206  208  211  214  227  232  238  273  275  278  287  291\n",
      "  302  303  304  307  311  313  317  322  324  325  342  344  353  358  359\n",
      "  366  371  378  383  388  396  399  400  407  409  423  427  429  432  433\n",
      "  434  437  440  444  450  453  463  479  481  492  496  508  509  511  513\n",
      "  514  523  526  529  533  536  540  542  546  549  561  565  566  578  579\n",
      "  581  586  588  597  604  609  616  618  620  627  629  631  640  648  649\n",
      "  656  684  687  698  700  703  715  716  720  727  732  735  736  748  751\n",
      "  756  761  765  768  771  776  782  783  797  810  812  816  818  826  829\n",
      "  836  837  845  846  847  849  853  855  856  859  867  889  894  895  896\n",
      "  903  904  905  912  917  921  927  934  944  946  965  966  971  973  975\n",
      "  977  983  993 1005 1013 1019 1024 1026 1034 1036 1045 1048 1051 1052 1053\n",
      " 1070 1073 1077 1082 1085 1096 1106 1118 1119 1120 1139 1140 1141 1145 1146\n",
      " 1149 1150 1157 1158 1160 1161 1172 1173 1174 1176 1180 1188 1191 1197 1199\n",
      " 1200 1203 1205 1219 1222 1237 1242 1244 1247 1256 1262 1263 1277 1283 1285\n",
      " 1286 1287 1290 1291 1293 1294 1296 1298 1302 1307 1312 1315 1318 1320 1325\n",
      " 1326 1335 1344 1345 1350 1357 1358 1362 1363 1366 1367 1369 1374 1375 1380\n",
      " 1388 1398 1399 1402 1405 1407 1411 1418 1423 1425 1426 1429 1432 1441 1449\n",
      " 1456 1464 1478 1482 1494 1500 1511 1516 1517 1522 1523 1530 1531 1537 1545\n",
      " 1560 1561 1563 1566 1584 1586 1587 1602 1606 1610 1613 1620 1628 1629 1631\n",
      " 1633 1634 1635 1637 1638 1640 1645 1653 1658 1668 1670 1676 1680 1681 1688\n",
      " 1707 1714 1715 1717 1719 1720 1722 1725 1726 1733 1735 1737 1739 1740 1744\n",
      " 1747 1754 1758 1762 1765 1766 1772 1780 1787 1796 1799 1818 1822 1825 1835\n",
      " 1838 1839 1843 1844 1845 1846 1852 1861 1866 1872 1873 1874 1880 1883 1884\n",
      " 1887 1892 1899 1900 1904 1905 1910 1911 1913 1915 1917 1922 1931 1933 1936\n",
      " 1937 1945 1947 1959 1960 1961 1968 1969 1990 1992 1994 1997 2009 2010 2013\n",
      " 2015 2023 2024 2027 2039 2042 2048 2049 2054 2055 2057 2066 2071 2074 2080\n",
      " 2081 2087 2109 2122 2124 2128 2130 2139 2147 2158 2161 2162 2165 2166 2169\n",
      " 2170 2174 2195 2196 2197 2204 2205 2211 2212 2217 2220 2229 2239 2240 2243\n",
      " 2261 2262 2265 2268 2280 2291 2305 2307 2313 2322 2327 2336 2347 2354 2362\n",
      " 2364 2373 2375 2387 2390 2398 2408 2410 2411 2417 2434 2436 2445 2456 2466\n",
      " 2469 2476 2478 2479 2485 2490 2491 2493 2496 2501 2503 2504 2506 2523 2525\n",
      " 2526 2530 2531 2532 2533 2536 2539 2540 2541 2545 2548 2549 2555 2559 2570\n",
      " 2576 2579 2590 2593 2596 2597 2599 2604 2605 2608 2609 2610 2612 2620 2629\n",
      " 2633 2634 2639 2640 2646 2659 2660 2664 2671 2674 2675 2678 2679 2684 2686\n",
      " 2688 2689 2693 2699 2702 2703 2707 2723 2729 2738 2742 2745 2748 2749 2753\n",
      " 2755 2762 2763 2767 2772 2775 2776 2784 2786 2788 2795 2819 2825 2826 2827\n",
      " 2830 2831 2841 2845 2868 2880 2887 2894 2896 2897 2899 2900] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d29a41b16c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0my_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predictions\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \"\"\"\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    503\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 505\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m                 for train, test in cv)\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m    140\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 273\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_counts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Hongsup/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                            max_leaf_nodes)\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sklearn.grid_search\n",
    "\n",
    "parameters = dict(feature_selection__k=[100, 200], \n",
    "              random_forest__n_estimators=[50, 100, 200],\n",
    "              random_forest__min_samples_split=[2, 3, 4, 5, 10])\n",
    "\n",
    "cv = sklearn.grid_search.GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "y_predictions = cv.predict(X_test)\n",
    "report = sklearn.metrics.classification_report( y_test, y_predictions )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV seems a little scary at first, because the parameter grid is easy to mess up. \n",
    "There’s **a particular convention** being followed in the way that **the parameters are named in the parameters dictionary.**\n",
    "- I need to have the name of the Pipeline step (e.g. feature_selection, not select; or random_forest, not clf), followed by **two underscores**, followed by the name of the parameter (in sklearn parlance) that I want to vary. \n",
    "\n",
    "To put this all together in a painfully simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "steps = [(\"my_classifier\", clf)]\n",
    "\n",
    "### “my_classifier” is the name of the random forest classifier in the steps list; \n",
    "### min_samples_split is the associated sklearn parameter that I want to vary\n",
    "parameters = dict{my_classifier__min_samples_split=[2, 3, 4, 5]}  \n",
    "pipe = Pipeline(steps)\n",
    "cv = GridSearchCV( pipe, param_grid = parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I’ve got the parameter grid set up properly, the power of GridSearchCV is that it multiplies out all the combinations of parameters and tries each one, making a 3-fold cross-validated model for each combination. Then I can ask for predictions from my GridSearchCV object and it will **automatically return to me the “best” set of predictions** (that is, the predictions from the best model that it tried), or I can explicitly ask for the best model/best parameters using methods associated with GridSearchCV. Of course, trying tons of models can be kind of time-consuming, but the outcome is a much better understanding of how my model performance depends on parameters.\n",
    "\n",
    "## GridSearchCV can be used as a single object.\n",
    "I should also mention that I can also use GridSearchCV on just a single object, rather than a full Pipeline. For example, I can optimize SelectKBest or the RandomForestClassifier on their own and that will work just fine. But since there can sometimes be interactions between various steps in the analysis, being able to optimize over the full Pipeline is really useful. It’s also trickier to do, which makes it a good example for teaching. \n",
    "- Last, GridSearchCV will **automatically cross validate all steps of the analysis**, such as the feature selection–it’s not just the final algorithm that should be cross-validated, but the upstream transforms as well!\n",
    "\n",
    "-----\n",
    "# Tutorial Summary\n",
    "This brings me to the end of this series, about **end-to-end data analysis in scikit-learn and pandas**. \n",
    "\n",
    "My goal in these posts is not to show a perfect analysis, or even one that demonstrates all the steps one might try, but instead to **focus on the process**. \n",
    "- If I can get something up and running quickly, even if it’s imperfect, I’m in a much better position to understand later on how much my refinements are indeed improving the analysis. \n",
    "- At the same time, there are definitely best practices and tools (like Pipeline and GridSearchCV) that will make my life much easier as my work expands. \n",
    "- Having a great set of tools in the python data science stack, and knowing when and how to deploy them, leaves me free to spend my time and energy on the most interesting, important and difficult-to-automate tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
