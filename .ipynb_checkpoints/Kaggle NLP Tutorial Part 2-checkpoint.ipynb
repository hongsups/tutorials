{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle NLP Tutorial: [Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors)\n",
    "-----\n",
    "# Part 2: Word Vectors\n",
    "# 1. Introducing Distributed Word Vectors\n",
    "[Word2vec](https://code.google.com/p/word2vec/), published by Google in 2013, is a neural network implementation that learns [distributed representations](http://www.cs.toronto.edu/~bonner/courses/2014s/csc321/lectures/lec5.pdf) for words.\n",
    "\n",
    "**1. Word2vec learns quickly relative to other models.**\n",
    "Other deep or recurrent neural network architectures had been proposed for learning word representations prior to this, but the major problem with these was the long time required to train the models. \n",
    "\n",
    "**2. Word2vec does not need labels.** If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics. \n",
    "- Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math. \n",
    "- The famous example is that, with highly trained word vectors, \"king - man + woman = queen.\"\n",
    "\n",
    "# 2. Using word2vec in Python\n",
    "- We use the **gensim** package.\n",
    "- Although Word2Vec does not require graphics processing units (GPUs) like many deep learning algorithms, it is **compute intensive**. Both Google's version and the Python version rely on multi-threading (running multiple processes in parallel on your computer to save time). \n",
    "- To train your model in a reasonable amount of time, you will need **cython**. Word2Vec will run without cython installed, but it will take days to run instead of minutes. (Cython is already included in Anaconda package).\n",
    "\n",
    "# 3. Preparing to Train a Model\n",
    "- Word2Vec can learn from unlabeled data.\n",
    "- We use unlabeledTrain.tsv, which contains 50,000 additional reviews with no labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"data/kaggle_nlp_labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv( \"data/kaggle_nlp_testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv( \"data/kaggle_nlp_unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "Similar to Part 1, but a couple of differences. \n",
    "- To train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. \n",
    "- For this reason, we will make stop word removal optional in the functions below. It also might be better not to remove numbers, but we leave that as an exercise for the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words, optionally removing stop words.  Returns a list of words.\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify input format: list of lists for Word2Vec\n",
    "Word2Vec expects single sentences, each one as a list of words -> a list of lists.\n",
    "- It is not at all straightforward how to split a paragraph into sentences. There are all kinds of gotchas in natural language. \n",
    "    - English sentences can end with \"?\", \"!\", \"\"\", or \".\", among other things\n",
    "    - Spacing and capitalization are not reliable guides either. \n",
    "- For this reason, we'll use NLTK's **punkt tokenizer for sentence splitting**. \n",
    "    - To use this, you will need to install NLTK and use nltk.download() to download the relevant training file for punkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "# nltk.download()   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. \n",
    "    # Returns a list of sentences, where each sentence is a list of words\n",
    "\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break down a review into sentences, and append\n",
    "**`+=`** vs **`append`** when it comes to **appending a list of lists to another list of lists**\n",
    "- **`append`**: only append the first list\n",
    "- **`+=`**: join all of the lists at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/bs4/__init__.py:182: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/bs4/__init__.py:182: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/Hongsup/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review.decode(\"utf8\"), tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review.decode(\"utf8\"), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n"
     ]
    }
   ],
   "source": [
    "# Check how many sentences we have in total - should be around 850,000+\n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'with', u'all', u'this', u'stuff', u'going', u'down', u'at', u'the', u'moment', u'with', u'mj', u'i', u've', u'started', u'listening', u'to', u'his', u'music', u'watching', u'the', u'odd', u'documentary', u'here', u'and', u'there', u'watched', u'the', u'wiz', u'and', u'watched', u'moonwalker', u'again']\n",
      "[u'maybe', u'i', u'just', u'want', u'to', u'get', u'a', u'certain', u'insight', u'into', u'this', u'guy', u'who', u'i', u'thought', u'was', u'really', u'cool', u'in', u'the', u'eighties', u'just', u'to', u'maybe', u'make', u'up', u'my', u'mind', u'whether', u'he', u'is', u'guilty', u'or', u'innocent']\n"
     ]
    }
   ],
   "source": [
    "print sentences[0]\n",
    "print sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training and Saving Your Model\n",
    "## Some [word2vec parameters](http://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "**Architecture**: skip-gram (default) or continuous bag of words \n",
    "- We found that skip-gram was very slightly slower but produced better results.\n",
    "\n",
    "**Training algorithm**: Hierarchical softmax (default) or negative sampling\n",
    "- Here, default\n",
    "\n",
    "**Downsampling of frequent words**\n",
    "- The Google documentation recommends values between .00001 and .001. \n",
    "- For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "\n",
    "**Word vector dimensionality**\n",
    "- More features result in longer runtimes, and often, but not always, result in better models. \n",
    "- Reasonable values can be in the tens to hundreds; we used 300.\n",
    "\n",
    "**Context / window size**: How many words of context should the training algorithm take into account? \n",
    "- 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "\n",
    "**Worker threads**: Number of parallel processes to run. \n",
    "- Computer-specific, but between 4 and 6 should work on most systems.\n",
    "\n",
    "**Minimum word count**: Limit the size of the vocabulary to meaningful words\n",
    "- Any word that does not occur at least this many times across all documents is ignored. \n",
    "- Reasonable values could be between 10 and 100. \n",
    "- In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. \n",
    "- This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time.\n",
    "\n",
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \n",
    "                          size=num_features, min_count = min_word_count, \n",
    "                          window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and save the model for later use. \n",
    "# You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the status\n",
    "Mac/Linux: use the \"top\" command from within Terminal to see if your system is successfully parallelizing while the model is training. With 4 workers, the first process in the list should be Python, and it should show 300-400% CPU usage. \n",
    "> `> top -o cpu`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exploring the Model Results\n",
    "## `doesnt_match()`: Deduce which word in a set is most dissimilar from the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More subtle difference: countries and cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`most_similar()`**: Get insight into the model's word clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.6276901960372925),\n",
       " (u'guy', 0.46386197209358215),\n",
       " (u'boy', 0.45082786679267883),\n",
       " (u'men', 0.43489524722099304),\n",
       " (u'lady', 0.4322325885295868),\n",
       " (u'person', 0.4219069182872772),\n",
       " (u'angus', 0.4098063111305237),\n",
       " (u'he', 0.4038394093513489),\n",
       " (u'himself', 0.4014739692211151),\n",
       " (u'girl', 0.3992985486984253)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'bee', 0.4868798851966858),\n",
       " (u'latifah', 0.4850972592830658),\n",
       " (u'princess', 0.48380163311958313),\n",
       " (u'king', 0.41025078296661377),\n",
       " (u'bride', 0.3812709450721741),\n",
       " (u'victoria', 0.38001295924186707),\n",
       " (u'queens', 0.36199265718460083),\n",
       " (u'belle', 0.3581508994102478),\n",
       " (u'mrs', 0.3525996506214142),\n",
       " (u'blonde', 0.34669917821884155)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.6613253951072693),\n",
       " (u'horrible', 0.6296415328979492),\n",
       " (u'dreadful', 0.5971604585647583),\n",
       " (u'atrocious', 0.5741333961486816),\n",
       " (u'abysmal', 0.5311987400054932),\n",
       " (u'horrendous', 0.5225872993469238),\n",
       " (u'laughable', 0.5124868750572205),\n",
       " (u'horrid', 0.5018720626831055),\n",
       " (u'lousy', 0.48499590158462524),\n",
       " (u'amateurish', 0.48359769582748413)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'lesbian', 0.5455188751220703),\n",
       " (u'homosexual', 0.5112344622612),\n",
       " (u'jewish', 0.4181421399116516),\n",
       " (u'women', 0.4166848957538605),\n",
       " (u'sensitive', 0.40602388978004456),\n",
       " (u'male', 0.3952866792678833),\n",
       " (u'heterosexual', 0.393507182598114),\n",
       " (u'bisexual', 0.3862374424934387),\n",
       " (u'queer', 0.38526836037635803),\n",
       " (u'community', 0.3794994354248047)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"gay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
